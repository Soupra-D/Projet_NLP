{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Necessary import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy\n",
    "#pip install pyLDAvis\n",
    "#conda install -c conda-forge spacy-model-en_core_web_md\n",
    "#conda install -c conda-forge spacy-model-en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\Program\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import secrets\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "import nltk\n",
    "import pyLDAvis\n",
    "\n",
    "import spacy\n",
    "import tomotopy as tp\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec, Phrases, KeyedVectors\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "# from pattern.en import pluralize, singularize\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from spacy.parts_of_speech import IDS as POS_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(\"../../data/News_Category_Dataset_v2.json\", lines=True, dtype={\"headline\": str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_word_split(texts):\n",
    "    \"\"\"Function identifying words in a sentence in a really dummy way.\n",
    "        \n",
    "        Argument:\n",
    "            - texts (list of str): a list of raw texts in which we'd like to identify words\n",
    "            \n",
    "        Return:\n",
    "            - list of list containing each word separately.\n",
    "    \"\"\"\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        texts_out.append(text.split(\" \"))\n",
    "        \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_occurences(texts):\n",
    "    \"\"\"You have to define this function yourself. \"\"\"\n",
    "    \n",
    "    words = itertools.chain.from_iterable(texts)\n",
    "    \n",
    "    word_count = pd.Series(words).value_counts()\n",
    "    word_count = pd.DataFrame({\"Word\": word_count.index, \"Count\": word_count.values})\n",
    "\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(texts):\n",
    "    \"\"\"Check wheter all the dataset is conform to the expected behaviour.\"\"\"\n",
    "    assert all([isinstance(t, str) for t in texts]), \"Input data contains something different than strings.\"\n",
    "    assert all([t != np.nan for t in texts]), \"Input data contains NaN values.\"\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_format(texts):\n",
    "    return [str(t) for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(texts_in):\n",
    "    \"\"\"Removes incorrect patterns from a list of texts, such as hyperlinks, bullet points and so on\"\"\"\n",
    "    \n",
    "    texts_out = re.sub(r'https?:\\/\\/[A-Za-z0-9_.-~\\-]*', ' ', texts_in, flags=re.MULTILINE)\n",
    "    texts_out = re.sub(r'[(){}\\[\\]<>]', ' ', texts_out, flags=re.MULTILINE)\n",
    "    texts_out = re.sub(r'&amp;#.*;', ' ', texts_out, flags=re.MULTILINE)\n",
    "    texts_out = re.sub(r'&gt;', ' ', texts_out, flags=re.MULTILINE)\n",
    "    texts_out = re.sub(r'â€™', \"'\", texts_out, flags=re.MULTILINE)\n",
    "    texts_out = re.sub(r'\\s+', ' ', texts_out, flags=re.MULTILINE)\n",
    "    texts_out = re.sub(r'&#x200B;', ' ', texts_out, flags=re.MULTILINE)\n",
    "    texts_out = re.sub(r'\\-', ' ', texts_out, flags=re.MULTILINE)\n",
    "    \n",
    "    # Mail regex\n",
    "    # This regex is correct but WAY TOO LONG to process. So we skip it with a simpler version\n",
    "    # texts_out = re.sub(r\"(?i)(?:[a-z0-9!#$%&'*+\\/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+\\/=?^_`{|}~-]+)*|\\\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\\\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\", '', texts_out, flags=re.MULTILINE)\n",
    "    texts_out = re.sub(r'[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+\\.[a-zA-Z0-9-_.]+', '', texts_out, flags=re.MULTILINE)\n",
    "    # Phone regex\n",
    "    # This regex is correct but WAY TOO LONG to process. So we skip it with a simpler version\n",
    "    # texts_out = re.sub(r\".*?(\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}).*?\", '', texts_out, flags=re.MULTILINE)\n",
    "    texts_out = re.sub(r\"\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}\", '', texts_out, flags=re.MULTILINE)\n",
    "    # Remove names in twitter\n",
    "    texts_out = re.sub(r'@\\S+( |\\n)', '', texts_out, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove starts commonly used on social media\n",
    "    texts_out = re.sub(r'\\*', '', texts_out, flags=re.MULTILINE)\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unifying texts & converting sentences to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    \"\"\"Converts sentences to words.\n",
    "\n",
    "    Convert sentences in lists of words while removing the accents and the punctuation.\n",
    "\n",
    "    @param:\n",
    "        sentences: a list of strings, the sentences we want to convert\n",
    "    @return\n",
    "        A list of words' lists.\n",
    "    \"\"\"\n",
    "    for sentence in tqdm(sentences):\n",
    "        yield (simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing useless words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords(additional_stopwords=[]):\n",
    "    \"\"\"Return a list of english stopwords, that can be augmented by using a stopwords file or a list of stopwords\n",
    "\n",
    "    Args:\n",
    "        filepath (str, optional): path to a text file where each line is a stopword\n",
    "        additional_stopwords (list of str, optional): list of string representing stopwords\n",
    "    Returns:\n",
    "        List of strings representing stopwords\n",
    "    \"\"\"\n",
    "    # Loading standard english stop words\n",
    "    with open('stopwords.txt', 'r') as f:\n",
    "        stop_w = f.readlines()\n",
    "    stopwords = [s.rstrip() for s in stop_w]\n",
    "\n",
    "    # Adding stop words from sklearn\n",
    "    stopwords = list(text.ENGLISH_STOP_WORDS.union(stopwords))\n",
    "\n",
    "    # Adding words from a list if specified\n",
    "    if additional_stopwords:\n",
    "        stopwords += additional_stopwords\n",
    "\n",
    "    # Removing duplicates\n",
    "    stopwords = list(set(stopwords))\n",
    "\n",
    "    # Removing some \\n that were included in the native stopwords of sklearn ... WHY?\n",
    "    stopwords = [s.replace(\"\\n\", \"\") for s in stopwords]\n",
    "\n",
    "    stopwords = sorted(stopwords, key=str.lower)\n",
    "\n",
    "    return stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigrams(texts, bigram_count=15, threshold=10, convert_sent_to_words=False, as_str=True):\n",
    "    \"\"\"Identify bigrams in texts and return the texts with bigrams integrated\"\"\"\n",
    "    if convert_sent_to_words:\n",
    "        texts = list(sent_to_words(texts))\n",
    "    \n",
    "    bigram_model = Phraser(Phrases(texts, min_count=bigram_count, threshold=threshold))\n",
    "    \n",
    "    if as_str:\n",
    "        return [\" \".join(bigram_model[t]) for t in texts]\n",
    "\n",
    "    else:\n",
    "        return [bigram_model[t] for t in texts]\n",
    "\n",
    "def create_trigrams(texts, trigram_count=15, threshold=10, convert_sent_to_words=False, as_str=True):\n",
    "    \"\"\"Identify trigrams in texts and return the texts with trigrams integrated\"\"\"\n",
    "    if convert_sent_to_words:\n",
    "        texts = list(sent_to_words(texts))\n",
    "    \n",
    "    bigram_model = Phraser(Phrases(texts, min_count=bigram_count, threshold=threshold))\n",
    "    \n",
    "    if as_str:\n",
    "        return [\" \".join(bigram_model[t]) for t in texts]\n",
    "\n",
    "    else:\n",
    "        return [bigram_model[t] for t in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming / Lemmatization & Part-of-Speech filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_texts(texts, \n",
    "                    allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'], \n",
    "                    forbidden_postags=[], \n",
    "                    as_sentence=False, \n",
    "                    get_postags=False, \n",
    "                    spacy_model=None):\n",
    "    \"\"\"Lemmatize a list of texts.\n",
    "    \n",
    "            Please refer to https://spacy.io/api/annotation for details on the allowed\n",
    "        POS tags.\n",
    "        @params:\n",
    "            - texts_in: a list of texts, where each texts is a string\n",
    "            - allowed_postags: a list of part of speech tags, in the spacy fashion\n",
    "            - as_sentence: a boolean indicating whether the output should be a list of sentences instead of a list of word lists\n",
    "        @return:\n",
    "            - A list of texts where each entry is a list of words list or a list of sentences\n",
    "        \"\"\"\n",
    "    texts_out = []\n",
    "    \n",
    "    if allowed_postags and forbidden_postags:\n",
    "        raise ValueError(\"Can't specify both allowed and forbidden postags\")\n",
    "\n",
    "    if forbidden_postags:\n",
    "        allowed_postags = list(set(POS_map.keys()).difference(set(forbidden_postags)))\n",
    "\n",
    "    if not spacy_model:\n",
    "        print(\"Loading spacy model\")\n",
    "        spacy_model = spacy.load('en_core_web_md')\n",
    "\n",
    "    print(\"Beginning lemmatization process\")\n",
    "    total_steps = len(texts)\n",
    "\n",
    "    docs = spacy_model.pipe(texts)\n",
    "\n",
    "    for i, doc in tqdm(enumerate(docs), total=total_steps):\n",
    "        if get_postags:\n",
    "            texts_out.append([\"_\".join([token.lemma_, token.pos_]) for token in doc if token.pos_ in allowed_postags])\n",
    "        else:\n",
    "            texts_out.append(\n",
    "                [token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    \n",
    "    if as_sentence:\n",
    "        texts_out = [\" \".join(text) for text in texts_out]\n",
    "        \n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained Word2Vec representations from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en_core_web_sm')\n",
    "#nlp(\"this is a course\")[3].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_word_embeddings(texts, occurences):\n",
    "#    words = []\n",
    "#    vector_representations = []\n",
    "#\n",
    "#    for word in occurences[\"Word\"].head(100):\n",
    "#        words.append(word)\n",
    "#        vector_representations.append(nlp(word)[0].vector)\n",
    "#\n",
    "#    return pd.DataFrame({\"Words\": words, \"Vector\": vector_representations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyser le corpus de texte pour en extraire ses caractéristiques spécifiques (taille moyenne, types de mots utilisés, mots les plus fréquents, stopwords, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Ensuring data quality.** You have to make sure that there's no N/A in your data and that everything is in the good format shape. Having this as the entrance of your pipeline will save you a lot of time in the long run, so try defining it thoroughly.\n",
    "\n",
    "\n",
    "2. **Filtering texts from unwanted characters**. Especially if you get data from web, you'll end up with HTML tags or encoding stuff that you don't need in your texts. Before applying anything to them, you need to get them cleaned up. Here, try removing the dates and the punctuation for instance.\n",
    "\n",
    "\n",
    "3. **Unify your texts**. (*This is topic modeling specific*). You don't want to make the difference between a word at the beginning of a phrase of in the middle of it here. You should unify all your words by lowercasing them and deaccenting them as well.\n",
    "\n",
    "\n",
    "4. **Converting sentences to lists of words**. Some words aren't needed for our analyses, such as *your*, *my*, etc. In order to remove them easily, you have to convert your sentences to lists of words. You can use the dummy function defined above but I'd advised against it. Try finding a function that does that smoothly in [gensim.utils](https://radimrehurek.com/gensim/utils.html)!\n",
    "\n",
    "\n",
    "5. **Remove useless words**. You need to remove useless words from your corpus. You have two approaches: [use a hard defined list of stopwords](https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/) or rely on TF-IDF to identify useless words. The first is the simplest, the second might yield better results!\n",
    "\n",
    "\n",
    "6. **Creating n-grams**. If you look at New York, it is composed of two words. As a result, a word count wouldn't really return a true count for *New York* per se. In NLP, we represent New York as New_York, which is considered a single word. The n-gram creation consists in identifying words that occur together often and regrouping them. It boosts interpretability for topic modeling in this case.\n",
    "\n",
    "\n",
    "7. **Stemming / Lemmatization**. Shouldn't run, running, runnable be grouped and counted as a single word when we're identifying discussion topics? Yes, they should. Stemming is the process of cutting words to their word root (run- for instance) quite brutally while lemmatization will do the same by identifying the kind of word it is working on. You should convert the corpus words into those truncated representations to have a more realistic word count.\n",
    "\n",
    "\n",
    "8. **Part of speech tagging**. POS helps in the identification of verbs, nouns, adjectives, etc. For topic models, it is a good idea to work only on verbs and nouns. Adjectives don't convey info about the actual underlying topic discussed at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring the data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>TECH</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>Reuters, Reuters</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200853 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             category                                           headline  \\\n",
       "0               CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1       ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2       ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3       ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4       ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "...               ...                                                ...   \n",
       "200848           TECH  RIM CEO Thorsten Heins' 'Significant' Plans Fo...   \n",
       "200849         SPORTS  Maria Sharapova Stunned By Victoria Azarenka I...   \n",
       "200850         SPORTS  Giants Over Patriots, Jets Over Colts Among  M...   \n",
       "200851         SPORTS  Aldon Smith Arrested: 49ers Linebacker Busted ...   \n",
       "200852         SPORTS  Dwight Howard Rips Teammates After Magic Loss ...   \n",
       "\n",
       "                 authors                                               link  \\\n",
       "0        Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1          Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2             Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3             Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4             Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "...                  ...                                                ...   \n",
       "200848  Reuters, Reuters  https://www.huffingtonpost.com/entry/rim-ceo-t...   \n",
       "200849                    https://www.huffingtonpost.com/entry/maria-sha...   \n",
       "200850                    https://www.huffingtonpost.com/entry/super-bow...   \n",
       "200851                    https://www.huffingtonpost.com/entry/aldon-smi...   \n",
       "200852                    https://www.huffingtonpost.com/entry/dwight-ho...   \n",
       "\n",
       "                                        short_description       date  \n",
       "0       She left her husband. He killed their children... 2018-05-26  \n",
       "1                                Of course it has a song. 2018-05-26  \n",
       "2       The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3       The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4       The \"Dietland\" actress said using the bags is ... 2018-05-26  \n",
       "...                                                   ...        ...  \n",
       "200848  Verizon Wireless and AT&T are already promotin... 2012-01-28  \n",
       "200849  Afterward, Azarenka, more effusive with the pr... 2012-01-28  \n",
       "200850  Leading up to Super Bowl XLVI, the most talked... 2012-01-28  \n",
       "200851  CORRECTION: An earlier version of this story i... 2012-01-28  \n",
       "200852  The five-time all-star center tore into his te... 2012-01-28  \n",
       "\n",
       "[200853 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the dataset passing our data quality check?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "source = force_format(dataset[\"headline\"])\n",
    "print(f\"Is the dataset passing our data quality check?\\n{check_data_quality(source)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV',\n",
       " \"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\",\n",
       " 'Hugh Grant Marries For The First Time At Age 57',\n",
       " \"Jim Carrey Blasts 'Castrato' Adam Schiff And Democrats In New Artwork\",\n",
       " 'Julianna Margulies Uses Donald Trump Poop Bags To Pick Up After Her Dog',\n",
       " \"Morgan Freeman 'Devastated' That Sexual Harassment Claims Could Undermine Legacy\",\n",
       " \"Donald Trump Is Lovin' New McDonald's Jingle In 'Tonight Show' Bit\",\n",
       " 'What To Watch On Amazon Prime That’s New This Week',\n",
       " \"Mike Myers Reveals He'd 'Like To' Do A Fourth Austin Powers Film\",\n",
       " 'What To Watch On Hulu That’s New This Week']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering texts from unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [filter_text(t) for t in source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV',\n",
       " \"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\",\n",
       " 'Hugh Grant Marries For The First Time At Age 57',\n",
       " \"Jim Carrey Blasts 'Castrato' Adam Schiff And Democrats In New Artwork\",\n",
       " 'Julianna Margulies Uses Donald Trump Poop Bags To Pick Up After Her Dog',\n",
       " \"Morgan Freeman 'Devastated' That Sexual Harassment Claims Could Undermine Legacy\",\n",
       " \"Donald Trump Is Lovin' New McDonald's Jingle In 'Tonight Show' Bit\",\n",
       " 'What To Watch On Amazon Prime That’s New This Week',\n",
       " \"Mike Myers Reveals He'd 'Like To' Do A Fourth Austin Powers Film\",\n",
       " 'What To Watch On Hulu That’s New This Week']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify your texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200853/200853 [00:04<00:00, 40902.07it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = list(sent_to_words(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['there',\n",
       "  'were',\n",
       "  'mass',\n",
       "  'shootings',\n",
       "  'in',\n",
       "  'texas',\n",
       "  'last',\n",
       "  'week',\n",
       "  'but',\n",
       "  'only',\n",
       "  'on',\n",
       "  'tv'],\n",
       " ['will',\n",
       "  'smith',\n",
       "  'joins',\n",
       "  'diplo',\n",
       "  'and',\n",
       "  'nicky',\n",
       "  'jam',\n",
       "  'for',\n",
       "  'the',\n",
       "  'world',\n",
       "  'cup',\n",
       "  'official',\n",
       "  'song'],\n",
       " ['hugh', 'grant', 'marries', 'for', 'the', 'first', 'time', 'at', 'age']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting sentences to lists of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stopwords = get_stopwords(additional_stopwords=[\"trump\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove useless words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "st_words = stopwords.words('english')\n",
    "st_words.extend(['trump', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200853/200853 [00:04<00:00, 49968.72it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = [[word for word in txt if word not in st_words] for txt in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mass', 'shootings', 'texas', 'last', 'week', 'tv'],\n",
       " ['smith',\n",
       "  'joins',\n",
       "  'diplo',\n",
       "  'nicky',\n",
       "  'jam',\n",
       "  'world',\n",
       "  'cup',\n",
       "  'official',\n",
       "  'song'],\n",
       " ['hugh', 'grant', 'marries', 'first', 'time', 'age']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = create_bigrams(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mass_shootings texas last week tv',\n",
       " 'smith joins diplo nicky jam world_cup official song',\n",
       " 'hugh grant marries first_time age']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy model\n",
      "Beginning lemmatization process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200853/200853 [03:06<00:00, 1075.68it/s]\n"
     ]
    }
   ],
   "source": [
    "l_texts = lemmatize_texts(texts,\n",
    "                allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'X', 'PROPN'], \n",
    "                #allowed_postags=['ADJ', 'VERB', 'ADV', 'X', 'PROPN'],\n",
    "                get_postags=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mass_shooting', 'texas', 'last', 'week', 'tv'],\n",
       " ['smith', 'join', 'diplo', 'nicky', 'jam', 'world_cup', 'official', 'song'],\n",
       " ['hugh', 'grant', 'marrie', 'first_time', 'age']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences = compute_word_occurences(l_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>photo</td>\n",
       "      <td>11840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new</td>\n",
       "      <td>6024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>make</td>\n",
       "      <td>5789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video</td>\n",
       "      <td>5729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>say</td>\n",
       "      <td>5272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word  Count\n",
       "0  photo  11840\n",
       "1    new   6024\n",
       "2   make   5789\n",
       "3  video   5729\n",
       "4    say   5272"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_l_texts = l_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pre-processing des données.\n",
    "\n",
    "Pour le préprocessing des données, nous avons utilisé les étapes suivantes : \n",
    "  - Suppression des caractères problématique ('-', '#', '@' etc) via les Regex\n",
    "  - Supperssion des stop words\n",
    "  - Conversion des phrases en liste de mots\n",
    "  - Création des n-grames\n",
    "  - Stemming / Lemmatization\n",
    "    \n",
    "Nous avons commencé par utiliser la méthode LDA.\n",
    "Pour cela nous avons cherché le nombre de topics en comptant les valeurs unique des catégories.\n",
    "\n",
    "Par la suite nous avons entrainé le modèle avec nos données.\n",
    "\n",
    "Un fois les topic généré, nous avon utilisé une matice de cohérence afin de vérifier la qualité des topic généré.\n",
    "Aux premiers essais, nous avions un taux de 0.3 à 0.35. \n",
    "Apres quelque réglages, nous avons pu monter à 0.43. De plus nous avons vérifié manuellement quelque valeurs pour nous rassuer (sanity check).\n",
    "\n",
    "Enfin, nous avons utilisé pyLDAvis afin de visualiser les topics trouvés afin de faciliter la comparaison avec un autre modèle de Topic Modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['category'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 200699 , Vocab size: 17459 , Num words: 1077817\n",
      "Removed top words: ['photo', 'new', 'make']\n"
     ]
    }
   ],
   "source": [
    "term_weight = tp.TermWeight.IDF\n",
    "#term_weight = tp.TermWeight.ONE\n",
    "\n",
    "lda = tp.LDAModel(tw=term_weight, min_cf=5, rm_top=3,  k=41, eta=0.01, alpha=0.1, seed=99999)\n",
    "\n",
    "#for vec in tqdm(l_texts):\n",
    "for vec in sa_l_texts:\n",
    "    if vec: \n",
    "        lda.add_doc(vec)\n",
    "        \n",
    "        \n",
    "# Initiate sampling burn-in  (i.e. discard 100 first iterations)\n",
    "lda.burn_in = 100\n",
    "lda.train(0)\n",
    "print('Num docs:', len(lda.docs), ', Vocab size:', lda.num_vocabs,\n",
    "      ', Num words:', lda.num_words)\n",
    "print('Removed top words:', lda.removed_top_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:39<02:51, 21.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\tLog-likelihood: -8.100781064905087\tNum. of topics: 40\n",
      "Iteration: 100\tLog-likelihood: -7.82703421495805\tNum. of topics: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:15<03:17, 28.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 200\tLog-likelihood: -7.809504355635744\tNum. of topics: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:46<02:56, 29.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 300\tLog-likelihood: -7.79425717954028\tNum. of topics: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [02:44<01:56, 29.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 400\tLog-likelihood: -7.782877241307491\tNum. of topics: 40\n",
      "Iteration: 500\tLog-likelihood: -7.7731110234960195\tNum. of topics: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [03:41<00:57, 28.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 600\tLog-likelihood: -7.764254549929908\tNum. of topics: 40\n",
      "Iteration: 700\tLog-likelihood: -7.757585797376632\tNum. of topics: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:44<00:00, 28.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 800\tLog-likelihood: -7.750018484554905\tNum. of topics: 40\n",
      "Iteration: 900\tLog-likelihood: -7.744539239833555\tNum. of topics: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, 1000, 100)):\n",
    "    lda.train(100) # 100 iterations at a time\n",
    "    print('Iteration: {}\\tLog-likelihood: {}\\tNum. of topics: {}'.format(i, lda.ll_per_word, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(hdp, top_n):\n",
    "    '''Wrapper function to extract topics from trained tomotopy HDP model \n",
    "    \n",
    "    ** Inputs **\n",
    "    hdp:obj -> HDPModel trained model\n",
    "    top_n: int -> top n words in topic based on frequencies\n",
    "    \n",
    "    ** Returns **\n",
    "    topics: dict -> per topic, an arrays with top words and associated frequencies \n",
    "    '''\n",
    "    \n",
    "    # Get most important topics by # of times they were assigned (i.e. counts)\n",
    "    sorted_topics = [k for k, v in sorted(enumerate(hdp.get_count_by_topics()), key=lambda x:x[1], reverse=True)]\n",
    "\n",
    "    topics=dict()\n",
    "    \n",
    "    # For topics found, extract only those that are still assigned\n",
    "    for k in sorted_topics:\n",
    "        #if not hdp.is_live_topic(k): continue # remove un-assigned topics at the end (i.e. not alive)\n",
    "        topic_wp =[]\n",
    "        for word, prob in hdp.get_topic_words(k, top_n=top_n):\n",
    "            topic_wp.append((word, prob))\n",
    "\n",
    "        topics[k] = topic_wp # store topic word/frequency array\n",
    "        \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsLDA = get_topics(lda, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topicsLDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('study', 0.011174843646585941),\n",
       " ('link', 0.008897550404071808),\n",
       " ('sleep', 0.007773944176733494),\n",
       " ('risk', 0.007630125153809786),\n",
       " ('brain', 0.006417974829673767)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicsLDA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build gensim objects\n",
    "vocab = corpora.Dictionary(sa_l_texts)\n",
    "corpus = [vocab.doc2bow(words) for words in sa_l_texts]\n",
    "\n",
    "# Build topic list from dictionary\n",
    "topic_list=[]\n",
    "for k, tups in topicsLDA.items():\n",
    "    topic_tokens=[]\n",
    "    for w, p in tups:\n",
    "        topic_tokens.append(w)\n",
    "\n",
    "    topic_list.append(topic_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43906047997481173"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = CoherenceModel(topics=topic_list, corpus=corpus, dictionary=vocab,\n",
    "                    texts=sa_l_texts, coherence='c_v')\n",
    "    \n",
    "cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\Program\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "topic_term_dists = np.stack([lda.get_topic_word_dist(k) for k in range(lda.k)])\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in lda.docs])\n",
    "doc_lengths = np.array([len(doc.words) for doc in lda.docs])\n",
    "vocab = list(lda.used_vocabs)\n",
    "term_frequency = lda.used_vocab_freq\n",
    "\n",
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists, \n",
    "    doc_topic_dists, \n",
    "    doc_lengths, \n",
    "    vocab, \n",
    "    term_frequency\n",
    ")\n",
    "pyLDAvis.save_html(prepared_data, 'ldavisModelLDA.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analyse de la LDA\n",
    "Avec la méthode LDA, on a un taux de cohgérence plutot faible avec 0.43.\n",
    "\n",
    "Cependant apres observation, on a des résultats cohérent dans les différents topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HDP Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 200699 , Vocab size: 17459 , Num words: 1077817\n",
      "Removed top words: ['photo', 'new', 'make']\n"
     ]
    }
   ],
   "source": [
    "term_weight = tp.TermWeight.IDF\n",
    "#term_weight = tp.TermWeight.ONE\n",
    "\n",
    "# alpha : concentration coeficient of Dirichlet Process for document-table\n",
    "# gamma : concentration coeficient of Dirichlet Process for table-topic\n",
    "\n",
    "hdp = tp.HDPModel(tw=term_weight, min_cf=5, rm_top=3, gamma=1, alpha=0.1, initial_k=5, seed=99999)\n",
    "\n",
    "#for vec in tqdm(l_texts):\n",
    "for vec in sa_l_texts:\n",
    "    if vec: \n",
    "        hdp.add_doc(vec)\n",
    "        \n",
    "        \n",
    "# Initiate sampling burn-in  (i.e. discard 100 first iterations)\n",
    "hdp.burn_in = 100\n",
    "hdp.train(0)\n",
    "print('Num docs:', len(hdp.docs), ', Vocab size:', hdp.num_vocabs,\n",
    "      ', Num words:', hdp.num_words)\n",
    "print('Removed top words:', hdp.removed_top_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:27<04:06, 27.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\tLog-likelihood: -8.17790771110098\tNum. of topics: 348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:35<09:15, 79.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100\tLog-likelihood: -8.0100987431796\tNum. of topics: 883\n",
      "Iteration: 200\tLog-likelihood: -7.930697343192925\tNum. of topics: 1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [07:05<07:57, 95.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 300\tLog-likelihood: -7.874164038738576\tNum. of topics: 1149\n",
      "Iteration: 400\tLog-likelihood: -7.831305800085842\tNum. of topics: 1242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [08:59<06:47, 101.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500\tLog-likelihood: -7.799515092712636\tNum. of topics: 1299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [10:56<05:20, 106.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 600\tLog-likelihood: -7.773441338020598\tNum. of topics: 1371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [17:57<00:00, 130.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 700\tLog-likelihood: -7.758421642215346\tNum. of topics: 1370\n",
      "Iteration: 800\tLog-likelihood: -7.738171685936314\tNum. of topics: 1461\n",
      "Iteration: 900\tLog-likelihood: -7.728638487760438\tNum. of topics: 1461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [17:57<00:00, 107.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "for i in tqdm(range(0, 1000, 100)):\n",
    "    hdp.train(100) # 100 iterations at a time\n",
    "    print('Iteration: {}\\tLog-likelihood: {}\\tNum. of topics: {}'.format(i, hdp.ll_per_word, hdp.live_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdp_topics(hdp, top_n):\n",
    "    '''Wrapper function to extract topics from trained tomotopy HDP model \n",
    "    \n",
    "    ** Inputs **\n",
    "    hdp:obj -> HDPModel trained model\n",
    "    top_n: int -> top n words in topic based on frequencies\n",
    "    \n",
    "    ** Returns **\n",
    "    topics: dict -> per topic, an arrays with top words and associated frequencies \n",
    "    '''\n",
    "    \n",
    "    # Get most important topics by # of times they were assigned (i.e. counts)\n",
    "    sorted_topics = [k for k, v in sorted(enumerate(hdp.get_count_by_topics()), key=lambda x:x[1], reverse=True)]\n",
    "\n",
    "    topics=dict()\n",
    "    \n",
    "    # For topics found, extract only those that are still assigned\n",
    "    for k in sorted_topics:\n",
    "        if not hdp.is_live_topic(k): continue # remove un-assigned topics at the end (i.e. not alive)\n",
    "        topic_wp =[]\n",
    "        for word, prob in hdp.get_topic_words(k, top_n=top_n):\n",
    "            topic_wp.append((word, prob))\n",
    "\n",
    "        topics[k] = topic_wp # store topic word/frequency array\n",
    "        \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_HDP = get_hdp_topics(hdp, top_n=5) # changing top_n changes no. of words displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source : Angry White Dude's Rant About People Speaking Spanish In NYC Goes Viral\n"
     ]
    }
   ],
   "source": [
    "k = 543\n",
    "print(\"source : \" + str(source[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1461"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics_HDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('video', 0.004791125655174255),\n",
       " ('home', 0.0034482572227716446),\n",
       " ('world', 0.003362217452377081),\n",
       " ('city', 0.0031991309951990843),\n",
       " ('day', 0.0026657807175070047)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_HDP[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdp.save('sample_hdp_model.bin')\n",
    "# Iteration: 900\tLog-likelihood: -7.124920004899671\tNum. of topics: 2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdp = tp.HDPModel.load('sample_hdp_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohérence et Inférence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build gensim objects\n",
    "\n",
    "vocab = corpora.Dictionary(sa_l_texts)\n",
    "corpus = [vocab.doc2bow(words) for words in sa_l_texts]\n",
    "\n",
    "# Build topic list from dictionary\n",
    "topic_list=[]\n",
    "for k, tups in topics_HDP.items():\n",
    "    topic_tokens=[]\n",
    "    for w, p in tups:\n",
    "        topic_tokens.append(w)\n",
    "\n",
    "    topic_list.append(topic_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6226497479782317"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = CoherenceModel(topics=topic_list, corpus=corpus, dictionary=vocab,\n",
    "                    texts=sa_l_texts, coherence='c_v')\n",
    "    \n",
    "cm.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Résultats Matrice de Cohérence\n",
    "En test sur une échantillon de 5000 headlind, nous avons pu monter à un taut de 0.66 env.\n",
    "Cependant, en vérifiant certains headline avec le topic associé manuellement, on se rend compte que les résultats n'étaient pas sattisfaisant.\n",
    "Apres d'autres essaie avec 100k headlines, nous avons un taut de 0.66 et les topics nous semblent plus logique que les résultats précédents.\n",
    "\n",
    "En utilisant la totalité des headlines (200k) nous avons un taut à 0.52. En vérifiant manuellement les résultats sont plus logiques.\n",
    "\n",
    "Ont aurraient put continuer à faire du fine tunning. Mais l'entrainnement avec 200k prennais 15-20 min/entrainnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id du topic ligne 6208 :\t 4\n",
      "\n",
      "Headline : Kevin Hart Drops An F-Bomb In Awkward NFL Network Interview\n",
      "Lematisation : ['kevin_hart', 'drop', 'bomb', 'awkward', 'nfl', 'network', 'interview']\n"
     ]
    }
   ],
   "source": [
    "#sarcasme ligne = 6208\n",
    "ligne = 6208\n",
    "#ligne = 65923\n",
    "\n",
    "\n",
    "test_doc = sa_l_texts[ligne]\n",
    "doc_inst = hdp.make_doc(test_doc)\n",
    "topic_dist, ll = hdp.infer(doc_inst)\n",
    "topic_idx = np.array(topic_dist).argmax()\n",
    "\n",
    "print(\"Id du topic ligne {} :\\t {}\".format(ligne, topic_idx))\n",
    "print(\"\\nHeadline : \" + source[ligne])\n",
    "print(\"Lematisation : \" + str(sa_l_texts[ligne]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('video', 0.00543650146573782),\n",
       " ('star', 0.0031008764635771513),\n",
       " ('get', 0.0028743334114551544),\n",
       " ('show', 0.002710863947868347),\n",
       " ('watch', 0.0025414691772311926),\n",
       " ('love', 0.0023660496808588505),\n",
       " ('woman', 0.002308787079527974),\n",
       " ('say', 0.0022705034352838993),\n",
       " ('look', 0.0021435802336782217),\n",
       " ('movie', 0.0021048341877758503)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdp.get_topic_words(topic_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category                                                        COMEDY\n",
      "headline             Kevin Hart Drops An F-Bomb In Awkward NFL Netw...\n",
      "short_description                                \"I've been drinking.\"\n",
      "Name: 6208, dtype: object\n",
      "\n",
      "https://www.huffingtonpost.com/entry/kevin-hart-nfl-f-bomb_us_5a7816f2e4b06ee97af49a0f\n"
     ]
    }
   ],
   "source": [
    "print(dataset.loc[ligne,[\"category\",\"headline\",\"short_description\"]])\n",
    "print('\\n' + dataset[\"link\"][ligne])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en cherchant à debud l'affichage via pyLDAvis\n",
    "# https://github.com/bab2min/tomotopy/issues/74\n",
    "#Avec tomotopy, il faut convertir \n",
    "\n",
    "ldamodel, _ = hdp.convert_to_lda(topic_threshold=0.001)\n",
    "\n",
    "# pyLDAvis에 입력하여 시각화하기\n",
    "topic_term_dists = np.stack([ldamodel.get_topic_word_dist(k) for k in range(ldamodel.k)])\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in ldamodel.docs])\n",
    "doc_lengths = np.array([len(doc.words) for doc in ldamodel.docs])\n",
    "vocab = list(ldamodel.used_vocabs)\n",
    "term_frequency = ldamodel.used_vocab_freq\n",
    "\n",
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists, \n",
    "    doc_topic_dists, \n",
    "    doc_lengths, \n",
    "    vocab, \n",
    "    term_frequency\n",
    ")\n",
    "pyLDAvis.save_html(prepared_data, 'ldavisModelHDP.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
